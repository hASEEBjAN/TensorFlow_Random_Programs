{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hASEEBjAN/TensorFlow_Random_Programs/blob/master/Cart-Pole%20Game%20using%20Reinforcement%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm4yDr8QxU49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcdf00b1-7af4-4026-e784-527518b8729a"
      },
      "source": [
        "#for random input generation\n",
        "import random\n",
        "#Re-Enforcement Learning algorthm Q learning in our case\n",
        "import gym\n",
        "#for mathematical operation\n",
        "import numpy as np\n",
        "#\n",
        "from collections import deque\n",
        "#here we use Q learning model from keras tha is Sequantial\n",
        "from keras.models import Sequential\n",
        "#having Dense layers weights\n",
        "from keras.layers import Dense\n",
        "#and Adam optimizer\n",
        "from keras.optimizers import Adam\n",
        "#for using of storing of data operating system based operations\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQI81K7YyBOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#making envirnment for cartpole game\n",
        "env=gym.make('CartPole-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJDwFxhMyaZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48ef5383-e8dd-40b8-ff0d-c6a495440eb4"
      },
      "source": [
        "#input  \n",
        "state_size=env.observation_space.shape[0]\n",
        "state_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6R27HgWy0_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3764046-5d6c-4c8b-edd4-4924ce5122e4"
      },
      "source": [
        "#output \n",
        "action_size=env.action_space.n\n",
        "action_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AwucX4sy-uW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for number of training in one iteration\n",
        "batch_size=32\n",
        "#number of iteration max\n",
        "n_episodes=1001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIRfZ-D1zSg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for storing file\n",
        "output_dir='carpolev0/outputfile'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR2IiPqAzt9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make a dir if not present\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1R5jYpG1JTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here we create the Qlearning class having all required procedure \n",
        "class DQNAgent:\n",
        "  #initializing by giving input and output size of envirnment\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size=state_size #input size of agent\n",
        "    self.action_size=action_size #output size of agent\n",
        "    self.memory=deque(maxlen=2000) #memory for storing random past states and rewards\n",
        "    self.gamma=0.95 #discount variable\n",
        "    self.epsilon=1.0 #initializing of value for rewards\n",
        "    self.decay_epsilon=0.995 #decay by this value for every lose\n",
        "    self.min_epsilon=0.01 #and run upto this value\n",
        "    self.learning_rate=0.001 #learning rate of program\n",
        "    self.model=self._build_model() #model creation\n",
        "  \n",
        "  def _build_model(self):\n",
        "    model=Sequential() #we are using sequential model\n",
        "    #input layer\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    #middle layer\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    #output layer\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    #gradient decent algorthm \n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "  \n",
        "  #for storing the past memory\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "  \n",
        "  #doing action on input state\n",
        "  def act(self, state):\n",
        "    #if random number is less than 1\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      #return that as action\n",
        "      return random.randrange(self.action_size)\n",
        "    #else predict action value using model created above\n",
        "    act_values=self.model.predict(state)\n",
        "    #return that action\n",
        "    return np.argmax(act_values[0])\n",
        "  \n",
        "  #for lose start again\n",
        "  def replay(self, batch_size):\n",
        "    #select a number of parallel nodes in batch size\n",
        "    minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "    #while all nodes are not done i-e complete task or fail\n",
        "    for state, action, reward, next_state, done in minibatch:\n",
        "      #give reward\n",
        "      target = reward\n",
        "      #predict the next_state\n",
        "      target_f= self.model.predict(state)\n",
        "      #if the action is same reward is same else increase reward\n",
        "      target_f[0][action]= reward if done else reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "      #predict the scenario using best fit regressor\n",
        "      self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "    #if fail or done than decrease epsilon value that is reward \n",
        "    if self.epsilon > self.min_epsilon:\n",
        "      self.epsilon *=self.decay_epsilon\n",
        "  #load weights to model\n",
        "  def load(self,name):\n",
        "    self.model.load_weights(name)\n",
        "  #save weights of model\n",
        "  def save(self,name):\n",
        "    self.model.save_weights(name)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoZvEp4M9foO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a3355a80-397f-4d74-c28e-fc776bd6c357"
      },
      "source": [
        "#create objecto of Deep Q Learning model and initialize it by givin input and output size\n",
        "agent=DQNAgent(state_size, action_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1G1L-sR__rD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "003cdcb1-09fa-4aa0-c820-439b852c84d1"
      },
      "source": [
        "agent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DQNAgent at 0x7f67c08e52e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0faH10o2ADgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afedd39a-c49b-4367-9ae4-abdfa30f955e"
      },
      "source": [
        "state=env.reset()\n",
        "print(state)\n",
        "state=np.reshape(state,[1,4])\n",
        "print(state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.03028655 -0.02894974  0.01990582  0.00473251]\n",
            "[[ 0.03028655 -0.02894974  0.01990582  0.00473251]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU1g2LC2AOhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5817
        },
        "outputId": "6309f643-5f4b-4558-9332-27fb1d82ad79"
      },
      "source": [
        "#scores queu store 100 pre scores\n",
        "scores = deque(maxlen=100)\n",
        "#start training\n",
        "for e in range(n_episodes):\n",
        "  #create random input\n",
        "  state = env.reset()\n",
        "  state=np.reshape(state,[1,state_size])\n",
        "  done = False\n",
        "  i = 0\n",
        "  #start iteration of batches\n",
        "  while not done:\n",
        "    #find an action of define state\n",
        "    action = agent.act(state)\n",
        "    #set next_step, reward and fail/pass value by the action of previous state input\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    next_state=np.reshape(next_state,[1, state_size])\n",
        "    #store the value of this operation\n",
        "    agent.remember(state, action, reward, next_state, done)\n",
        "    #update previous input \n",
        "    state = next_state\n",
        "    #if completed\n",
        "    if done:\n",
        "      #print value of this try\n",
        "      print('episode: {}/{}, score: {}, e: {:.2}'.format(e,n_episodes, np.mean(scores), agent.epsilon))\n",
        "      #if passed congrate\n",
        "      if np.mean(scores) >= 195 and e >= 100:\n",
        "        print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
        "        #you completed the task\n",
        "        done=True\n",
        "        break\n",
        "      break\n",
        "    #next iterator\n",
        "    i+=1\n",
        "  #congrates\n",
        "  if np.mean(scores)>=195 and e>=100:\n",
        "    break\n",
        "  #if fail\n",
        "  #enque scores\n",
        "  scores.append(i)\n",
        "  #and play again\n",
        "  agent.replay(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/1001, score: nan, e: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "episode: 1/1001, score: 37.0, e: 0.99\n",
            "episode: 2/1001, score: 38.0, e: 0.99\n",
            "episode: 3/1001, score: 28.0, e: 0.99\n",
            "episode: 4/1001, score: 27.0, e: 0.98\n",
            "episode: 5/1001, score: 30.0, e: 0.98\n",
            "episode: 6/1001, score: 13.0, e: 0.97\n",
            "episode: 7/1001, score: 17.0, e: 0.97\n",
            "episode: 8/1001, score: 47.0, e: 0.96\n",
            "episode: 9/1001, score: 20.0, e: 0.96\n",
            "episode: 10/1001, score: 19.0, e: 0.95\n",
            "episode: 11/1001, score: 14.0, e: 0.95\n",
            "episode: 12/1001, score: 52.0, e: 0.94\n",
            "episode: 13/1001, score: 19.0, e: 0.94\n",
            "episode: 14/1001, score: 12.0, e: 0.93\n",
            "episode: 15/1001, score: 12.0, e: 0.93\n",
            "episode: 16/1001, score: 17.0, e: 0.92\n",
            "episode: 17/1001, score: 24.0, e: 0.92\n",
            "episode: 18/1001, score: 21.0, e: 0.91\n",
            "episode: 19/1001, score: 14.0, e: 0.91\n",
            "episode: 20/1001, score: 75.0, e: 0.9\n",
            "episode: 21/1001, score: 28.0, e: 0.9\n",
            "episode: 22/1001, score: 12.0, e: 0.9\n",
            "episode: 23/1001, score: 13.0, e: 0.89\n",
            "episode: 24/1001, score: 16.0, e: 0.89\n",
            "episode: 25/1001, score: 19.0, e: 0.88\n",
            "episode: 26/1001, score: 27.0, e: 0.88\n",
            "episode: 27/1001, score: 15.0, e: 0.87\n",
            "episode: 28/1001, score: 10.0, e: 0.87\n",
            "episode: 29/1001, score: 28.0, e: 0.86\n",
            "episode: 30/1001, score: 21.0, e: 0.86\n",
            "episode: 31/1001, score: 13.0, e: 0.86\n",
            "episode: 32/1001, score: 13.0, e: 0.85\n",
            "episode: 33/1001, score: 12.0, e: 0.85\n",
            "episode: 34/1001, score: 13.0, e: 0.84\n",
            "episode: 35/1001, score: 16.0, e: 0.84\n",
            "episode: 36/1001, score: 14.0, e: 0.83\n",
            "episode: 37/1001, score: 16.0, e: 0.83\n",
            "episode: 38/1001, score: 13.0, e: 0.83\n",
            "episode: 39/1001, score: 20.0, e: 0.82\n",
            "episode: 40/1001, score: 15.0, e: 0.82\n",
            "episode: 41/1001, score: 27.0, e: 0.81\n",
            "episode: 42/1001, score: 36.0, e: 0.81\n",
            "episode: 43/1001, score: 16.0, e: 0.81\n",
            "episode: 44/1001, score: 13.0, e: 0.8\n",
            "episode: 45/1001, score: 23.0, e: 0.8\n",
            "episode: 46/1001, score: 14.0, e: 0.79\n",
            "episode: 47/1001, score: 23.0, e: 0.79\n",
            "episode: 48/1001, score: 36.0, e: 0.79\n",
            "episode: 49/1001, score: 12.0, e: 0.78\n",
            "episode: 50/1001, score: 12.0, e: 0.78\n",
            "episode: 51/1001, score: 16.0, e: 0.77\n",
            "episode: 52/1001, score: 21.0, e: 0.77\n",
            "episode: 53/1001, score: 16.0, e: 0.77\n",
            "episode: 54/1001, score: 8.0, e: 0.76\n",
            "episode: 55/1001, score: 13.0, e: 0.76\n",
            "episode: 56/1001, score: 24.0, e: 0.76\n",
            "episode: 57/1001, score: 19.0, e: 0.75\n",
            "episode: 58/1001, score: 14.0, e: 0.75\n",
            "episode: 59/1001, score: 14.0, e: 0.74\n",
            "episode: 60/1001, score: 16.0, e: 0.74\n",
            "episode: 61/1001, score: 11.0, e: 0.74\n",
            "episode: 62/1001, score: 9.0, e: 0.73\n",
            "episode: 63/1001, score: 14.0, e: 0.73\n",
            "episode: 64/1001, score: 49.0, e: 0.73\n",
            "episode: 65/1001, score: 11.0, e: 0.72\n",
            "episode: 66/1001, score: 10.0, e: 0.72\n",
            "episode: 67/1001, score: 17.0, e: 0.71\n",
            "episode: 68/1001, score: 15.0, e: 0.71\n",
            "episode: 69/1001, score: 19.0, e: 0.71\n",
            "episode: 70/1001, score: 17.0, e: 0.7\n",
            "episode: 71/1001, score: 9.0, e: 0.7\n",
            "episode: 72/1001, score: 14.0, e: 0.7\n",
            "episode: 73/1001, score: 10.0, e: 0.69\n",
            "episode: 74/1001, score: 29.0, e: 0.69\n",
            "episode: 75/1001, score: 26.0, e: 0.69\n",
            "episode: 76/1001, score: 10.0, e: 0.68\n",
            "episode: 77/1001, score: 21.0, e: 0.68\n",
            "episode: 78/1001, score: 28.0, e: 0.68\n",
            "episode: 79/1001, score: 20.0, e: 0.67\n",
            "episode: 80/1001, score: 12.0, e: 0.67\n",
            "episode: 81/1001, score: 15.0, e: 0.67\n",
            "episode: 82/1001, score: 13.0, e: 0.66\n",
            "episode: 83/1001, score: 39.0, e: 0.66\n",
            "episode: 84/1001, score: 13.0, e: 0.66\n",
            "episode: 85/1001, score: 10.0, e: 0.65\n",
            "episode: 86/1001, score: 12.0, e: 0.65\n",
            "episode: 87/1001, score: 11.0, e: 0.65\n",
            "episode: 88/1001, score: 14.0, e: 0.64\n",
            "episode: 89/1001, score: 10.0, e: 0.64\n",
            "episode: 90/1001, score: 11.0, e: 0.64\n",
            "episode: 91/1001, score: 17.0, e: 0.63\n",
            "episode: 92/1001, score: 17.0, e: 0.63\n",
            "episode: 93/1001, score: 10.0, e: 0.63\n",
            "episode: 94/1001, score: 17.0, e: 0.62\n",
            "episode: 95/1001, score: 15.0, e: 0.62\n",
            "episode: 96/1001, score: 49.0, e: 0.62\n",
            "episode: 97/1001, score: 20.0, e: 0.61\n",
            "episode: 98/1001, score: 12.0, e: 0.61\n",
            "episode: 99/1001, score: 14.0, e: 0.61\n",
            "episode: 100/1001, score: 26.0, e: 0.61\n",
            "episode: 101/1001, score: 11.0, e: 0.6\n",
            "episode: 102/1001, score: 26.0, e: 0.6\n",
            "episode: 103/1001, score: 19.0, e: 0.6\n",
            "episode: 104/1001, score: 20.0, e: 0.59\n",
            "episode: 105/1001, score: 18.0, e: 0.59\n",
            "episode: 106/1001, score: 27.0, e: 0.59\n",
            "episode: 107/1001, score: 25.0, e: 0.58\n",
            "episode: 108/1001, score: 99.0, e: 0.58\n",
            "episode: 109/1001, score: 19.0, e: 0.58\n",
            "episode: 110/1001, score: 21.0, e: 0.58\n",
            "episode: 111/1001, score: 23.0, e: 0.57\n",
            "episode: 112/1001, score: 15.0, e: 0.57\n",
            "episode: 113/1001, score: 18.0, e: 0.57\n",
            "episode: 114/1001, score: 12.0, e: 0.56\n",
            "episode: 115/1001, score: 16.0, e: 0.56\n",
            "episode: 116/1001, score: 18.0, e: 0.56\n",
            "episode: 117/1001, score: 78.0, e: 0.56\n",
            "episode: 118/1001, score: 25.0, e: 0.55\n",
            "episode: 119/1001, score: 18.0, e: 0.55\n",
            "episode: 120/1001, score: 9.0, e: 0.55\n",
            "episode: 121/1001, score: 11.0, e: 0.55\n",
            "episode: 122/1001, score: 8.0, e: 0.54\n",
            "episode: 123/1001, score: 13.0, e: 0.54\n",
            "episode: 124/1001, score: 9.0, e: 0.54\n",
            "episode: 125/1001, score: 12.0, e: 0.53\n",
            "episode: 126/1001, score: 11.0, e: 0.53\n",
            "episode: 127/1001, score: 8.0, e: 0.53\n",
            "episode: 128/1001, score: 9.0, e: 0.53\n",
            "episode: 129/1001, score: 11.0, e: 0.52\n",
            "episode: 130/1001, score: 10.0, e: 0.52\n",
            "episode: 131/1001, score: 8.0, e: 0.52\n",
            "episode: 132/1001, score: 7.0, e: 0.52\n",
            "episode: 133/1001, score: 30.0, e: 0.51\n",
            "episode: 134/1001, score: 26.0, e: 0.51\n",
            "episode: 135/1001, score: 15.0, e: 0.51\n",
            "episode: 136/1001, score: 19.0, e: 0.51\n",
            "episode: 137/1001, score: 14.0, e: 0.5\n",
            "episode: 138/1001, score: 9.0, e: 0.5\n",
            "episode: 139/1001, score: 40.0, e: 0.5\n",
            "episode: 140/1001, score: 31.0, e: 0.5\n",
            "episode: 141/1001, score: 47.0, e: 0.49\n",
            "episode: 142/1001, score: 21.0, e: 0.49\n",
            "episode: 143/1001, score: 23.0, e: 0.49\n",
            "episode: 144/1001, score: 23.0, e: 0.49\n",
            "episode: 145/1001, score: 16.0, e: 0.48\n",
            "episode: 146/1001, score: 16.0, e: 0.48\n",
            "episode: 147/1001, score: 12.0, e: 0.48\n",
            "episode: 148/1001, score: 15.0, e: 0.48\n",
            "episode: 149/1001, score: 41.0, e: 0.47\n",
            "episode: 150/1001, score: 13.0, e: 0.47\n",
            "episode: 151/1001, score: 12.0, e: 0.47\n",
            "episode: 152/1001, score: 24.0, e: 0.47\n",
            "episode: 153/1001, score: 13.0, e: 0.46\n",
            "episode: 154/1001, score: 19.0, e: 0.46\n",
            "episode: 155/1001, score: 9.0, e: 0.46\n",
            "episode: 156/1001, score: 12.0, e: 0.46\n",
            "episode: 157/1001, score: 10.0, e: 0.46\n",
            "episode: 158/1001, score: 11.0, e: 0.45\n",
            "episode: 159/1001, score: 10.0, e: 0.45\n",
            "episode: 160/1001, score: 21.0, e: 0.45\n",
            "episode: 161/1001, score: 9.0, e: 0.45\n",
            "episode: 162/1001, score: 10.0, e: 0.44\n",
            "episode: 163/1001, score: 9.0, e: 0.44\n",
            "episode: 164/1001, score: 17.0, e: 0.44\n",
            "episode: 165/1001, score: 11.0, e: 0.44\n",
            "episode: 166/1001, score: 24.0, e: 0.44\n",
            "episode: 167/1001, score: 14.0, e: 0.43\n",
            "episode: 168/1001, score: 18.0, e: 0.43\n",
            "episode: 169/1001, score: 20.0, e: 0.43\n",
            "episode: 170/1001, score: 14.0, e: 0.43\n",
            "episode: 171/1001, score: 14.0, e: 0.42\n",
            "episode: 172/1001, score: 12.0, e: 0.42\n",
            "episode: 173/1001, score: 26.0, e: 0.42\n",
            "episode: 174/1001, score: 90.0, e: 0.42\n",
            "episode: 175/1001, score: 10.0, e: 0.42\n",
            "episode: 176/1001, score: 12.0, e: 0.41\n",
            "episode: 177/1001, score: 13.0, e: 0.41\n",
            "episode: 178/1001, score: 68.0, e: 0.41\n",
            "episode: 179/1001, score: 31.0, e: 0.41\n",
            "episode: 180/1001, score: 48.0, e: 0.41\n",
            "episode: 181/1001, score: 23.0, e: 0.4\n",
            "episode: 182/1001, score: 38.0, e: 0.4\n",
            "episode: 183/1001, score: 15.0, e: 0.4\n",
            "episode: 184/1001, score: 49.0, e: 0.4\n",
            "episode: 185/1001, score: 50.0, e: 0.4\n",
            "episode: 186/1001, score: 58.0, e: 0.39\n",
            "episode: 187/1001, score: 20.0, e: 0.39\n",
            "episode: 188/1001, score: 61.0, e: 0.39\n",
            "episode: 189/1001, score: 78.0, e: 0.39\n",
            "episode: 190/1001, score: 49.0, e: 0.39\n",
            "episode: 191/1001, score: 20.0, e: 0.38\n",
            "episode: 192/1001, score: 29.0, e: 0.38\n",
            "episode: 193/1001, score: 77.0, e: 0.38\n",
            "episode: 194/1001, score: 28.0, e: 0.38\n",
            "episode: 195/1001, score: 31.0, e: 0.38\n",
            "episode: 196/1001, score: 24.0, e: 0.37\n",
            "episode: 197/1001, score: 18.0, e: 0.37\n",
            "episode: 198/1001, score: 26.0, e: 0.37\n",
            "episode: 199/1001, score: 73.0, e: 0.37\n",
            "episode: 200/1001, score: 38.0, e: 0.37\n",
            "episode: 201/1001, score: 47.0, e: 0.37\n",
            "episode: 202/1001, score: 36.0, e: 0.36\n",
            "episode: 203/1001, score: 40.0, e: 0.36\n",
            "episode: 204/1001, score: 26.0, e: 0.36\n",
            "episode: 205/1001, score: 22.0, e: 0.36\n",
            "episode: 206/1001, score: 21.0, e: 0.36\n",
            "episode: 207/1001, score: 19.0, e: 0.35\n",
            "episode: 208/1001, score: 26.0, e: 0.35\n",
            "episode: 209/1001, score: 35.0, e: 0.35\n",
            "episode: 210/1001, score: 27.0, e: 0.35\n",
            "episode: 211/1001, score: 22.0, e: 0.35\n",
            "episode: 212/1001, score: 27.0, e: 0.35\n",
            "episode: 213/1001, score: 34.0, e: 0.34\n",
            "episode: 214/1001, score: 20.0, e: 0.34\n",
            "episode: 215/1001, score: 24.0, e: 0.34\n",
            "episode: 216/1001, score: 25.0, e: 0.34\n",
            "episode: 217/1001, score: 30.0, e: 0.34\n",
            "episode: 218/1001, score: 44.0, e: 0.34\n",
            "episode: 219/1001, score: 31.0, e: 0.33\n",
            "episode: 220/1001, score: 40.0, e: 0.33\n",
            "episode: 221/1001, score: 29.0, e: 0.33\n",
            "episode: 222/1001, score: 30.0, e: 0.33\n",
            "episode: 223/1001, score: 121.0, e: 0.33\n",
            "episode: 224/1001, score: 102.0, e: 0.33\n",
            "episode: 225/1001, score: 24.0, e: 0.32\n",
            "episode: 226/1001, score: 30.0, e: 0.32\n",
            "episode: 227/1001, score: 48.0, e: 0.32\n",
            "episode: 228/1001, score: 17.0, e: 0.32\n",
            "episode: 229/1001, score: 30.0, e: 0.32\n",
            "episode: 230/1001, score: 61.0, e: 0.32\n",
            "episode: 231/1001, score: 44.0, e: 0.31\n",
            "episode: 232/1001, score: 104.0, e: 0.31\n",
            "episode: 233/1001, score: 93.0, e: 0.31\n",
            "episode: 234/1001, score: 55.0, e: 0.31\n",
            "episode: 235/1001, score: 80.0, e: 0.31\n",
            "episode: 236/1001, score: 62.0, e: 0.31\n",
            "episode: 237/1001, score: 66.0, e: 0.3\n",
            "episode: 238/1001, score: 48.0, e: 0.3\n",
            "episode: 239/1001, score: 23.0, e: 0.3\n",
            "episode: 240/1001, score: 25.0, e: 0.3\n",
            "episode: 241/1001, score: 56.0, e: 0.3\n",
            "episode: 242/1001, score: 29.0, e: 0.3\n",
            "episode: 243/1001, score: 28.0, e: 0.3\n",
            "episode: 244/1001, score: 24.0, e: 0.29\n",
            "episode: 245/1001, score: 19.0, e: 0.29\n",
            "episode: 246/1001, score: 15.0, e: 0.29\n",
            "episode: 247/1001, score: 13.0, e: 0.29\n",
            "episode: 248/1001, score: 16.0, e: 0.29\n",
            "episode: 249/1001, score: 27.0, e: 0.29\n",
            "episode: 250/1001, score: 15.0, e: 0.29\n",
            "episode: 251/1001, score: 9.0, e: 0.28\n",
            "episode: 252/1001, score: 14.0, e: 0.28\n",
            "episode: 253/1001, score: 35.0, e: 0.28\n",
            "episode: 254/1001, score: 66.0, e: 0.28\n",
            "episode: 255/1001, score: 42.0, e: 0.28\n",
            "episode: 256/1001, score: 25.0, e: 0.28\n",
            "episode: 257/1001, score: 30.0, e: 0.28\n",
            "episode: 258/1001, score: 36.0, e: 0.27\n",
            "episode: 259/1001, score: 63.0, e: 0.27\n",
            "episode: 260/1001, score: 23.0, e: 0.27\n",
            "episode: 261/1001, score: 26.0, e: 0.27\n",
            "episode: 262/1001, score: 22.0, e: 0.27\n",
            "episode: 263/1001, score: 25.0, e: 0.27\n",
            "episode: 264/1001, score: 31.0, e: 0.27\n",
            "episode: 265/1001, score: 69.0, e: 0.26\n",
            "episode: 266/1001, score: 30.0, e: 0.26\n",
            "episode: 267/1001, score: 31.0, e: 0.26\n",
            "episode: 268/1001, score: 49.0, e: 0.26\n",
            "episode: 269/1001, score: 101.0, e: 0.26\n",
            "episode: 270/1001, score: 36.0, e: 0.26\n",
            "episode: 271/1001, score: 66.0, e: 0.26\n",
            "episode: 272/1001, score: 55.0, e: 0.26\n",
            "episode: 273/1001, score: 39.0, e: 0.25\n",
            "episode: 274/1001, score: 40.0, e: 0.25\n",
            "episode: 275/1001, score: 60.0, e: 0.25\n",
            "episode: 276/1001, score: 36.0, e: 0.25\n",
            "episode: 277/1001, score: 31.0, e: 0.25\n",
            "episode: 278/1001, score: 86.0, e: 0.25\n",
            "episode: 279/1001, score: 35.0, e: 0.25\n",
            "episode: 280/1001, score: 158.0, e: 0.25\n",
            "episode: 281/1001, score: 103.0, e: 0.24\n",
            "episode: 282/1001, score: 29.0, e: 0.24\n",
            "episode: 283/1001, score: 67.0, e: 0.24\n",
            "episode: 284/1001, score: 33.0, e: 0.24\n",
            "episode: 285/1001, score: 41.0, e: 0.24\n",
            "episode: 286/1001, score: 112.0, e: 0.24\n",
            "episode: 287/1001, score: 104.0, e: 0.24\n",
            "episode: 288/1001, score: 81.0, e: 0.24\n",
            "episode: 289/1001, score: 44.0, e: 0.23\n",
            "episode: 290/1001, score: 15.0, e: 0.23\n",
            "episode: 291/1001, score: 23.0, e: 0.23\n",
            "episode: 292/1001, score: 34.0, e: 0.23\n",
            "episode: 293/1001, score: 88.0, e: 0.23\n",
            "episode: 294/1001, score: 72.0, e: 0.23\n",
            "episode: 295/1001, score: 140.0, e: 0.23\n",
            "episode: 296/1001, score: 86.0, e: 0.23\n",
            "episode: 297/1001, score: 26.0, e: 0.23\n",
            "episode: 298/1001, score: 26.0, e: 0.22\n",
            "episode: 299/1001, score: 24.0, e: 0.22\n",
            "episode: 300/1001, score: 57.0, e: 0.22\n",
            "episode: 301/1001, score: 39.0, e: 0.22\n",
            "episode: 302/1001, score: 45.0, e: 0.22\n",
            "episode: 303/1001, score: 37.0, e: 0.22\n",
            "episode: 304/1001, score: 90.0, e: 0.22\n",
            "episode: 305/1001, score: 77.0, e: 0.22\n",
            "episode: 306/1001, score: 55.0, e: 0.22\n",
            "episode: 307/1001, score: 21.0, e: 0.21\n",
            "episode: 308/1001, score: 36.0, e: 0.21\n",
            "episode: 309/1001, score: 56.0, e: 0.21\n",
            "episode: 310/1001, score: 33.0, e: 0.21\n",
            "episode: 311/1001, score: 58.0, e: 0.21\n",
            "episode: 312/1001, score: 76.0, e: 0.21\n",
            "episode: 313/1001, score: 127.0, e: 0.21\n",
            "episode: 314/1001, score: 64.0, e: 0.21\n",
            "episode: 315/1001, score: 58.0, e: 0.21\n",
            "episode: 316/1001, score: 66.0, e: 0.21\n",
            "episode: 317/1001, score: 31.0, e: 0.2\n",
            "episode: 318/1001, score: 65.0, e: 0.2\n",
            "episode: 319/1001, score: 174.0, e: 0.2\n",
            "episode: 320/1001, score: 50.0, e: 0.2\n",
            "episode: 321/1001, score: 55.0, e: 0.2\n",
            "episode: 322/1001, score: 47.0, e: 0.2\n",
            "episode: 323/1001, score: 50.0, e: 0.2\n",
            "episode: 324/1001, score: 92.0, e: 0.2\n",
            "episode: 325/1001, score: 90.0, e: 0.2\n",
            "episode: 326/1001, score: 57.0, e: 0.2\n",
            "episode: 327/1001, score: 49.0, e: 0.19\n",
            "episode: 328/1001, score: 45.0, e: 0.19\n",
            "episode: 329/1001, score: 128.0, e: 0.19\n",
            "episode: 330/1001, score: 49.0, e: 0.19\n",
            "episode: 331/1001, score: 199.0, e: 0.19\n",
            "Ran 331 episodes. Solved after 231 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}